Enclosed in the metadata_model folder is the code to recreate the model in py, ipynb, and pdf format. The images folder has all 5000 images used to train the spectral clustering model, the csv's of the metadata used in the model, and the rmd file used to scrape the images from Flickr API.

Our dataset consists of 500 RGB images for each of the following 10 topics: canyons, castles, coasts, fish, flowers, horses, planes, skylines, sunsets, and Yosemite. These photos were taken between January 1, 2016, and December 31, 2018, and posted on the image-sharing website Flickr. We retrieved these photos by using the photo_search and download_images functions in the R package “photosearcher” (Fox et al.), and doing quality control on all 5000 images making sure that we removed and replaced photos with inaccurate tags and noise. In addition to the images, this package returns a cleaned data frame of 62 columns of metadata for each image, ranging from the textual description the user gave the images, to the number of views the image had at the time of download.


We aimed to create a model that uses metadata to rank labeled images in a meaningful way. For example, on Instagram, if you look up any location or hashtag, it will return the images in reverse chronological order, and it will also return the “top” images taken at that location or with that hashtag. How do you decide what the best images within a desired label are? We solved this in a simplified version of a technique that Flickr uses. First, we clustered the data using scikit-learn Spectral Clustering using a Gaussian Kernel and KMeans Clustering. Then we found the cluster with the most images with the desired tag, and returned the images with the most views, limiting one photo per author to decrease any bias towards any one particular large creator. Flickr considers the frequency a photo appears (in our case the number of views) to be an indicator of how much users like the photo, and therefore if a lot of users like a photo it must be a good photo for that category. However, simply returning the photos with the most views is not a good strategy. The photos with the most views are not necessarily good representations of their respective tags, as they could be photos taken by famous creators with incorrect or misleading tags. By first clustering the images and finding the cluster with the most images of our desired tag, we are finding the most representative images of the tag and filtering out images that do not fit the tag as well. 

The model combining spectral clustering and metadata is difficult to assess. The spectral clustering was run on all 5000 images since no test images are necessary. The goal of this model was to return the best images from each topic. By the very nature of the question, there are no right or wrong solutions as it is a subjective question to answer. Unlike the other two models, we cannot use precision, as since we have access to the metadata, we will only be returning images from the desired topic. It does appear that the model only returns quality, representative images for each topic, but whether they are the best images cannot be said.  One way we can get some insight into how our model performs is by introducing some noise into the data. Looking at the model precision for each topic when noise is introduced, it is clear that performance varies with each topic, and that an overall precision of 0.74 when just 10% noise is introduced is not great performance. One main reason for this is the variance in the average number of views for each topic. For example, the average photo gets around 641 views, but the average picture of a canyon gets only 130 views, while the average picture of a sunset gets 982. This means that even if a chosen cluster is mostly high-quality pictures of canyons, it is unlikely for them to be chosen since they have such fewer views compared to the introduced noise. 

We can remove this bias against topics with fewer views by adjusting the number of views our noise has. When the noise is adjusted to have their views equal the 90th percentile of the desired label’s views, the resulting precision rises to 0.95, with 1 incorrect coast image, 3 incorrect horse images, and 1 incorrect Yosemite image.

Similar to prior models, another cause of error in our models are visually similar images/topics. Looking at the example results above, you can come up with some explanations for why incorrect photos were included in the cluster and ultimately chosen. For example, the incorrect coast image is a flat landscape at sunset that looks similar to the ocean, and the incorrect Yosemite image is a picture of trees at sunset that looks similar to a rock formation at sunset. Considering there was 10% noise added with views in the top 10% for each topic, 0.95 precision is a fairly good performance. 


When running the python code, the addresses for the photo directory and csv's have to be adjusted. Similary, the code that takes the image tag and image id from the image location may need to be adjusted based on the number of /'s before the image tag and /'s before the image id in the image location.
